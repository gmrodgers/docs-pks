---
title: Enterprise PKS Release Notes
owner: PKS
topictype: releasenotes
---

<strong><%= modified_date %></strong>

This topic contains release notes for <%= vars.product_full %> v1.4.x.

## <a id="1.4.1"></a>v1.4.1

**Release Date**: June 20, 2019

### <a id="v1.4.1-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.4.1</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>June 20, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.4.2+, v2.5.x, v2.6.x</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions (Azure only)</td>
        <td>v2.4.2, v2.4.3, v2.4.13, v2.5.5 and later, v2.6.x</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v250.25 <br><strong>Note:</strong> To address the Zombieload CVE, upload stemcell v250.48.<br> For more information, see the <a href="https://community.pivotal.io/s/article/mitigating-zombieload-for-pivotal-container-service-pks">Mitigating Zombieload for Pivotal Container Service (PKS)</a> KB article.</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.13.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.26.0</td>
    </tr>
    <tr>
        <td>NSX-T versions</td>
        <td>v2.3.1, v2.4.0.1, v2.4.1 (recommended) </td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">docker-boshrelease</a></td>
    </tr>
    <tr>
        <td>BBR version</td>
        <td>v1.8.0</td>
    </tr>
</table>

<p class="note warning"><strong>Warning:</strong> NSX-T v2.4 implements a new password expiration policy.
By default, the administrator password expires after 90 days. If the password expires,
you cannot deploy new Kubernetes clusters using PKS with NSX-T. For more information, see the
following <a href="https://kb.vmware.com/s/article/70691">VMware KB article</a>
and <a href="./nsxt-deploy-24.html#nsxt-24-password">Updating the NSX Manager Password for NSX-T v2.4</a> in the
<%= vars.product_short %> documentation.</p>

<p class="note"><strong>Note:</strong> NSX-T v2.4 implements a new user interface (UI) based on the NSX Policy API.
PKS v1.4.1 does not support the NSX Policy API. Any objects created through the new UI cannot be used with PKS v1.4.1.
If you are installing PKS v1.4.1 with NSX-T v2.4.x or upgrading to PKS v1.4.1 and NSX-T 2.4.x,
you must use the <strong>Advanced Networking</strong> tab in NSX Manager to create, read, update, and delete
all networking objects required for <%= vars.product_short %>. For more information, see <a href="./nsxt-deploy-24.html#ui-nsxt-24">NSX-T v2.4 Management Interfaces</a> in the
<%= vars.product_short %> documentation.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

If you are installing <%= vars.product_short %> on vSphere or on vSphere with NSX-T Data Center,
see the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a> for compatibility information.</p>

### <a id="v1.4.1-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.

### <a id="v1.4.1-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.4.1 are from PKS v1.4.0 and PKS v1.3.2 and later.

<p class="note warning"><strong>WARNING:</strong> Before you upgrade to PKS 1.4.x, you must increase the size of persistent disk for the PKS VM. 
    For more information, 
    see <a href="#not-enough-diskspace">Upgrade Fails Due to Insufficient Disk Space</a> in the Known Issues.</p>

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

<p class="note breaking"><strong>Breaking change:</strong> Upgrading to Ops Manager v2.6 on vSphere requires that you set a public SSH key in the OVF template prior to the upgrade. For more information, see <a href="#no-password-om">Passwords Not Supported for Ops Manager VM on vSphere</a> in the Breaking Changes section.</p>

### <a id="v1.4.1-whats-new"></a>What's New

<%= vars.product_short %> v1.4.1 adds the following:

* Support for VMware NSX-T v2.4.1.
For more information, see [VMware NSX-T Data Center 2.4.1 Release Notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4.1/rn/VMware-NSX-T-Data-Center-241-Release-Notes.html).
* Support for VMware NCP v2.4.1.
For more information, see [NSX Container Plugin 2.4 Release Notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/rn/NSX-Container-Plugin-Release-Notes.html) in the VMware documentation.
* Support for provisioning an NSX-T load balancer in front of the NSX management cluster.
For more information, see [Provisioning a Load Balancer for the NSX-T v2.4 Management Cluster](./nsxt-mgmt-lb.html).
* Ability to tune NCP rate limits.
* Configurable Kubernetes service network CIDR range. For more information, see [Networking](./installing-nsx-t.html#networking)
in _Installing Enterprise PKS on vSphere with NSX-T_.
* Configurable node IP block. For more information, see [Defining Network Profiles](./network-profiles-define.html).
* Improved functionality for the `/v1beta1/cluster` API endpoint to allow users to list and resize clusters that have compute profiles associated with them.
For more information, see [List Clusters with Compute Profile](./compute-profiles.html#list-clusters) and [Resize a Cluster with a Compute Profile](./compute-profiles.html#resize) in _Using Compute Profile (vSphere Only)_.

### <a id="v1.4.1-breaking-changes"></a> Breaking Changes

<%= vars.product_short %> v1.4.1 includes the following breaking changes:

#### <a name='no-password-om'></a> Passwords Not Supported for Ops Manager VM on vSphere

<%= vars.product_short %> v1.4.1 supports Ops Manager v2.6. 

Starting in Ops Manager v2.6, you can only SSH onto the Ops Manager VM in a vSphere deployment with a private SSH key. You cannot SSH onto the Ops Manager VM with a password.

To avoid upgrade failure and errors when authenticating, add a public key to the **Customize Template** screen of the the OVF template for the Ops Manager VM. Then, use the private key to SSH onto the Ops Manager VM. 

<p class="note warning"><strong>Warning</strong>: You cannot upgrade to Ops Manager v2.6 successfully without adding a public key. If you do not add a key, Ops Manager shuts down automatically because it cannot find a key and may enter a reboot loop.</p>

For more information about adding a public key to the OVF template, see [Deploy Ops Manager](https://docs.pivotal.io/pivotalcf/2-6/om/vsphere/deploy.html#deploy) in _Deploying Ops Manager on vSphere_. 

#### <a name='log-sink-entry-change'></a> Log Sink Entry Format Change

Log sink entries in PKS v1.4 are tagged with the human-readable cluster name instead of the host ID of the BOSH-defined VM.</p>

### <a id="v1.4.1-known-issues"></a> Known Issues

<%= vars.product_short %> v1.4.1 has the following known issues:

#### <a name="vm-sizes"></a> PKS-provisioned Kubernetes cluster creation fails in a vSphere with NSX-T environment 

**Symptom**

Kubernetes cluster creation fails with Enterprise PKS installed on vSphere with NSX-T.

**Explanation**

When configuring a plan for Kubernetes clusters, for the **Worker VM Type** setting, selecting either of the following Worker VM Types results in cluster creation failure because there is not enough disk to handle swap space. 

- `medium (cpu: 2, ram: 4 GM, disk: 8 GB)`
- `medium.mem (cpu: 1, ram: 8 GM, disk: 8 GB)`

BOSH will assign the same amount of RAM to swap up to 50% of the ephemeral disk size. The article [How much Swap Space is Allocated for BOSH-Deployed VMs](https://community.pivotal.io/s/article/How-much-Swap-Space-is-Allocated-for-BOSH-Deployed-VMs) describes the required swap space for VM types. Given the swap space requirements, the `medium` and `medium.mem` Worker VM types are only getting 4GB usable ephemeral disk which is not enough for successful cluster deployment of the PKS tile with NCP. 

**Workaround**

When selecting the Worker VM Type for Worker nodes, do not use the `medium` or `medium.mem` type in a vSphere with NSX-T environment. Use the default Worker VM Type `medium.disk (cpu: 2, ram: 4 GB, disk: 32 GB)` or any other Worker VM Type **except** `medium` or `medium.mem`. 

#### <a name="failed-clusters"></a> Upgrade All Service Instances Errand Fails

**Symptom**

After clicking **Apply Changes** in Ops Manager, 
the `upgrade-all-service-instances` errand fails.

**Explanation**

The `upgrade-all-service-instances` errand fails if one or more 
of your existing clusters are in a **FAILED** state.

To check cluster status, run `pks clusters`.

**Workaround**

If you experience this issue, delete and re-create the failed cluster. 
Follow the procedure in 
[Cannot Re-Create a Cluster that Failed to Deploy](troubleshoot-issues.html#cluster-recreate-fails).

#### <a name="not-enough-diskspace"></a> Upgrade Fails Due to Insufficient Disk Space

**Symptom**

The upgrade from PKS v1.3.x to v1.4.x fails with the following error in the BOSH debug log:

`1 of 8 pre-start scripts failed. Failed Jobs:pxc-mysql`

The pxc-mysql pre-start logs also includes the error message:

`panic: Cannot continue, insufficient disk space to complete migration`

**Explanation**

The upgrade to PKS v1.4.x includes a MySQL migration from MaridDB (Galera) to MySQL (Percona).
This process includes a backup of existing MySQL files.

When `/var/vcap/store` does not have enough space for the backups, the PKS upgrade fails.

**Workaround**

Prior to upgrading to PKS v1.4.x, increase the persistent disk type configured for the PKS VM.

1. In Ops Manager, click on the PKS tile and select **Resource Config**.

1. In **Pivotal Container Service**, select a larger disk type than the currently configured by using the **Persistent Disk Type** dropdown.
For example, if you have 10 GB currently selected, select 20 GB. 
 
If you experience this issue after attempting an upgrade, increase the disk store size and start the upgrade again. 

## <a id="v1.4.0"></a>v1.4.0

**Release Date**: April 25, 2019

### <a id="v1.4.0-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.4.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>April 25, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.4.2+, v2.5.x</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions (Azure only)</td>
        <td>v2.4.2, v2.4.3, v2.4.13, v2.5.5 and later</td>
    </tr>
    <tr>
        <td>Stemcell version</td>
        <td>v250.25</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.13.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.26.0</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.3.1, v2.4.0.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.0</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce<br><a href="https://github.com/cloudfoundry-incubator/docker-boshrelease/">CFCR</a></td>
    </tr>
    <tr>
        <td>BBR version</td>
        <td>v1.8.0</td>
    </tr>
</table>

<p class="note"><strong>WARNING:</strong> NSX-T v2.4 implements a new <em>password expiration policy</em>. 
By default the administrator password expires after 90 days. If the password expires, 
you cannot deploy new Kubernetes clusters using PKS with NSX-T. For more information, refer to the
following VMware KB article: <a href="https://kb.vmware.com/s/article/70691">https://kb.vmware.com/s/article/70691</a>. 
See also <a href="./nsxt-deploy-24.html#nsxt-24-password">Updating the NSX Manager Password for NSX-T v2.4</a> in the 
<%= vars.product_short %> documentation.</p>

<p class="note"><strong>Note:</strong> NSX-T v2.4 implements a new user interface (UI) based on the NSX Policy API. 
PKS v1.4.0 does not support the NSX Policy API. Any objects created via the new Policy-based UI cannot be used with PKS v1.4.0.
If you are installing PKS v1.4.0 with NSX-T v2.4.x, or upgrading to PKS v1.4.0 and NSX-T 2.4.x, 
you must use the "Advanced Networking" tab in NSX Manager to create, read, update, and delete 
all networking objects required for PKS. See also <a href="./nsxt-deploy-24.html#ui-nsxt-24">NSX-T v2.4 Management Interfaces</a> in the 
<%= vars.product_short %> documentation.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

For <%= vars.product_short %> installations on vSphere or on vSphere with NSX-T Data Center, refer to the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a>.</p>

### <a id="v1.4.0-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.

### <a id="v1.4.0-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.4.0 are from PKS v1.3.2 and later.

<p class="note warning"><strong>WARNING:</strong> Before you upgrade to PKS 1.4.x, you must increase the size of persistent disk for the PKS VM. 
    For more information, 
    see <a href="#not-enough-diskspace">Upgrade Fails Due to Insufficient Disk Space</a> in the Known Issues.</p>

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

When upgrading to NSX-T 2.4:

- Use the official VMware NSX-T Data Center 2.4 build.
- Apply the NSX-T v2.4.0.1 hot-patch. For more information, see [KB article 67499](https://kb.vmware.com/s/article/67449) in the VMware Knowledge Base.
- To obtain the NSX-T v2.4.0.1 hot-patch, open a support ticket with VMware Global Support Services (GSS) for NSX-T Engineering.

### <a id="v1.4.0-whats-new"></a>What's New

<%= vars.product_short %> v1.4.0 adds the following:

* Operators can configure up to ten sets of resource types, or _plans_, in the Enterprise PKS tile. All
plans except the first can made available or unavailable to developers deploying clusters. Plan 1
must be configured and made available as a default for developers. For more information, see the _Plans_ section of the installation topic for your IaaS, such as [Installing <%= vars.product_short %> on vSphere with NSX-T](./installing-nsx-t.html#plans).

* Operators can deploy up to 5 master nodes per plan. For more information, see the _Plans_ section of the installation topic for your IaaS, such as [Installing <%= vars.product_short %> on vSphere with NSX-T](./installing-nsx-t.html#plans).

* Operators can install PKS and Pivotal Application Service (PAS) on the same instance of Ops
Manager.

* Improved workflow for managing cluster access. For more information, see [Grant Cluster Access](manage-users.html#cluster-access) in _Managing Users in <%= vars.product_short %> with UAA_.

* Operators can create webhook ClusterSink resources. A webhook ClusterSink resource batches logs into 1 second units, wraps the resulting payload in JSON, and uses the POST method to deliver the logs to the address of your log management service. For more information see, [Create a Webhook ClusterSink Resource with YAML and kubectl](create-sinks.html#webhook-cluster-sink) in _Creating Sinks_.

* Operators can set quotas for maximum memory and CPU utilization in a PKS deployment. For more information, see [Managing Resource Usage](resource-usage.html). This is a beta feature.
    <%= partial 'beta-component' %>

* Operators can enable the PodSecurityPolicy admission plugin on a per-plan basis requiring cluster users to have policy, role, and role binding permissions to deploy pod workloads. See [Pod Security Policies](./pod-security-policy.html) for more information.

* Operators can enable the SecurityContextDeny admission plugin on a per-plan basis to prohibit the use of security context configurations on pods and containers. See [Security Context Deny](./security-context-deny.html) for more information.

* Operators can enable the DenyEscalatingExec admission plugin on a per-plan basis to prohibit the use of certain commands for containers that allow host access. See [Deny Escalating Execution](./deny-escalating-execution.html) for more information.

* Operators using vSphere can use HostGroups to define Availability Zones (AZs) for clusters in BOSH. See [Using vSphere Host Groups](./vsphere-host-group.html).

* Operators using vSphere can configure compute profiles to specify which vSphere
resources are used when deploying Kubernetes clusters in a PKS deployment.
For more information, see [Using Compute Profiles (vSphere Only)](compute-profiles.html).
    <%= partial 'beta-component' %>

* Operators using vSphere with NSX-T can update a Network Profile and add to or reorder the **Pods IP Block** IDs. For more information, see the [Change the Network Profile for a Cluster](./network-profiles.html#update-profile) section of _Using Network Profiles (NSX-T Only)_.

### <a id="v1.4.0-known-issues"></a>Breaking Changes and Known Issues

<p class="note breaking"><strong>Breaking change:</strong> Log sink entries in PKS v1.4 are tagged with the human-readable cluster name instead of the host ID of the BOSH-defined VM.</p>

<%= vars.product_short %> v1.4.0 has the following known issues:

#### <a name="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, in <%= vars.product_short %> v1.4, the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.

#### <a id='first-az'></a>Cluster Creation Fails When First AZ Runs out of Resources

**Symptom**

If the first availability zone (AZ) used by a plan with multiple AZs runs out of
resources, cluster creation fails with an error like the following:

<pre class="terminal">
L Error: CPI error 'Bosh::Clouds::CloudError' with message 'No valid placement found for requested memory: 4096
</pre>

**Explanation**

BOSH creates VMs for your <%= vars.product_short %> deployment using a round-robin
algorithm, creating the first VM in the first AZ that your plan uses.
If the AZ runs out of resources, cluster creation fails because BOSH cannot create
the cluster VM.

For example, if your three AZs each have enough resources for ten VMs, and you
create two clusters with four worker VMs each, BOSH creates VMs in the
following AZs:

<table>
  <tr>
    <th></th>
    <th>AZ1</th>
    <th>AZ2</th>
    <th>AZ3</th>
  </tr>
  <tr>
    <th>Cluster 1</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <th>Cluster 2</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
</table>

In this scenario, AZ1 has twice as many VMs as AZ2 or AZ3.

#### <a id='azure-worker-comm'></a>Azure Worker Node Communication Fails after Upgrade

**Symptom**

Outbound communication from a worker node VM fails after an upgrade to <%= vars.product_short %> v1.4.0.

**Explanation**

<%= vars.product_short %> 1.4.0 uses Azure Availability Sets to improve the uptime of workloads and worker nodes in the event of Azure platform failures. Worker node
VMs are distributed evenly across Availability Sets.

Azure Standard SKU Load Balancers are recommended for the Kubernetes control plane and Kubernetes ingress and egress. This load balancer type provides an IP address for outbound communication using SNAT.

During an upgrade, when BOSH rebuilds a given worker instance in an Availability Set,
Azure can time out while re-attaching the worker node network interface to the
back-end pool of the Standard SKU Load Balancer.

For more information, see [Outbound connections in Azure](https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections) in the Azure documentation.

**Workaround**

You can manually re-attach the worker instance to the back-end pool of the Azure Standard SKU Load Balancer in your Azure console.

#### <a name="not-enough-diskspace"></a> Upgrade Fails Due to Insufficient Disk Space

**Symptom**

The upgrade from PKS v1.3.x to v1.4.x fails with the following error in the BOSH debug log:

`1 of 8 pre-start scripts failed. Failed Jobs:pxc-mysql`

The pxc-mysql pre-start logs also includes the error message:

`panic: Cannot continue, insufficient disk space to complete migration`

**Explanation**

The upgrade to PKS v1.4.x includes a MySQL migration from MaridDB (Galera) to MySQL (Percona).
This process includes a backup of existing MySQL files.

When `/var/vcap/store` does not have enough space for the backups, the PKS upgrade fails.

**Workaround**

Prior to upgrading to PKS v1.4.x, increase the persistent disk type configured for the PKS VM.

1. In Ops Manager, click on the PKS tile and select **Resource Config**.

1. In **Pivotal Container Service**, select a larger disk type than the currently configured by using the **Persistent Disk Type** dropdown.
For example, if you have 10 GB currently selected, select 20 GB. 
 
If you experience this issue after attempting an upgrade, increase the disk store size and start the upgrade again.